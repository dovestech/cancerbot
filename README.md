Project Overview
================

Our web crawler looks for cancer related information in publically available information (forums, tweets/twitter, and other sources). We are targetting data that is related to cancer stages, medicines (drugs), and how people are responding to certain types of cancer. We are delivering the data-sets to scientists/doctors in the community so that they can use it for valuable research purposes.

We're also conducting human language processing efforts using a project known as FrameNet. The FrameNet project is building a lexical database of English that is both human-readable and machine-readable, based on annotating examples of how words are used in actual texts. From the student's point of view, it is a dictionary of more than 10,000 word senses, most of them with annotated examples that show the meaning and usage. For the researcher in Natural Language Processing, the more than 170,000 manually annotated sentences provide a unique training dataset for semantic role labeling, used in applications such as information extraction, machine translation, event recognition, sentiment analysis, etc.

Our cancer processing architecture has to sift through sentences and paragraphs, looking for relevance and keywords in the data. For example, a tweet: "Its day 3 on $drug, I am starting to feel itchy" We add the data to our dataset that states that there was a individual case of itchyness on day 3 while using the drug $drug. We then take this information and add a score / weight and do comparisons between other tweets that may have said the same.

The community can download a client version of Cancerbot to help with the process of crawling the internet for this type of data. Our web crawlers are programmed to obey all robots.txt and will work through the URL seed (xml file) located on our server. This project is open source and will allow users to commit to the software. This software is in BETA testing and may or may not function correctly at all times. Users will be able to upload the results of there web crawling efforts to our website so that we can perform analytics on the data using our Hadoop cluster. 